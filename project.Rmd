---
title: "BST 222 Final Project"
output:
  word_document: default
  html_document: default
authors: Yishan Zheng, Rindala Fayyad, Lauren Mock, Daniel Herrera, Yuning Liu, Jinyi Che
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Write a function for each simulation

STEP 1

Simulate data
- generate true beta values
- generate random noise (error terms) for each row/observation
- sample Xi's from a jointly Uniform distribution
- calculate Y values for each vector of X's using true betas and error terms
- result: simulated dataset

STEP 2

- fit a linear regression model on this simulated dataset
- result: empirical beta estimates 

```{r}
# write a function to use for each simulation
simulation <- function(n, d) {
  betas <- runif(d + 1, min = 0, max = 10) # simulate d+1 beta coefficients
  dat <- matrix(nrow = n, ncol = d + 1) # create empty data frame with n rows and d+1 columns
  dat <- as.data.frame(dat)
  e_vec <- rnorm(n, mean = 0, sd = 10) # vector for error terms (random noise)
  
  # now use these betas to simulate data (known underlying distribution)
  for (i in 1:n) {
    x_vec <- runif(d, min = 0, max = 100) # simulate a vector of X values from U[0,100]
    y <- sum(betas[2 : length(betas)] * x_vec) + betas[1] + e_vec[i] # Y = B0 + B1X1 + B2X2 + ... + error
    row <- c(y, x_vec) 
    dat[i,] <- row # fill in the data frame row by row with y and the vector of X values
  }
  
  # run a linear regression model on the simulated data
  colnames(dat)[1] <- "Y" # name first column "Y" (considered the outcome)
  lreg <- lm(Y ~ ., data = dat) # run a linear regression model to predict Y given the Xs
  estimate <- summary(lreg)$coeff[,1] # calculate regression coefficients (betas)
  return (estimate) # return betas
}
```


STEP 3

- repeat steps 1 and 2 1000 times for each desired value of d (the number of predictors)
- calculate MSE for all estimators
- look for trends in MSE

For d = 1:

```{r}

set.seed(17)
d = 1
n = 100
betas1 <- matrix(nrow = 1000, ncol = d + 1)
betas1 <- as.data.frame(betas1)

# run the simulation with n and d 1000 times
for (i in 1:1000){
  betas1[i,] <- simulation(n, d)
}

# empty data frame
var1 <- matrix(nrow = 1, ncol = d + 1)
var1 <- as.data.frame(var1)

# calculate variance of each estimator
var1[,1] <- var(betas1[,1])
for (i in 1: d + 1){
  var1[,i] <- var(betas1[,i])
}

head(betas1)
head(var1)

```


For d = 50

```{r}
set.seed(17)
d = 50
n = 100
betas50 <- matrix(nrow = 1000, ncol = d + 1)
betas50 <- as.data.frame(betas50)

# run the simulation with n and d 1000 times
for (i in 1:1000){
  betas50[i,] <- simulation(n, d)
}

# empty data frame
var50 <- matrix(nrow = 1, ncol = d + 1)
var50 <- as.data.frame(var50)

# calculate variance of each estimator
var50[,1] <- var(betas50[,1])
for (i in 1: d + 1){
  var50[,i] <- var(betas50[,i])
}

head(betas50)
head(var50)
```

For d = 75:

```{r}
set.seed(17)
d = 75
n = 100
betas75 <- matrix(nrow = 1000, ncol = d + 1)
betas75 <- as.data.frame(betas75)

# run the simulation with n and d 1000 times
for (i in 1:1000){
  betas75[i,] <- simulation(n, d)
}

# empty data frame
var75 <- matrix(nrow = 1, ncol = d + 1)
var75 <- as.data.frame(var75)

# calculate variance of each estimator
var75[,1] <- var(betas75[,1])
for (i in 1: d + 1){
  var75[,i] <- var(betas75[,i])
}

head(betas75)
head(var75)

```

For d = 97:

```{r}
set.seed(17)
d = 97
n = 100
betas97 <- matrix(nrow = 1000, ncol = d + 1)
betas97 <- as.data.frame(betas97)

# run the simulation with n and d 1000 times
for (i in 1:1000){
  betas97[i,] <- simulation(n, d)
}

# empty data frame
var97 <- matrix(nrow = 1, ncol = d + 1)
var97 <- as.data.frame(var97)

# calculate variance of each estimator
var97[,1] <- var(betas97[,1])
for (i in 1: d + 1){
  var97[,i] <- var(betas97[,i])
}

head(betas97)
head(var97)

```


For d = 98:

```{r}
set.seed(17)
d = 98
n = 100
betas98 <- matrix(nrow = 1000, ncol = d + 1)
betas98 <- as.data.frame(betas98)

# run the simulation with n and d 1000 times
for (i in 1:1000){
  betas98[i,] <- simulation(n, d)
}

# empty data frame
var98 <- matrix(nrow = 1, ncol = d + 1)
var98 <- as.data.frame(var98)

# calculate variance of each estimator
var98[,1] <- var(betas98[,1])
for (i in 1: d + 1){
  var98[,i] <- var(betas98[,i])
}

head(betas98)
head(var98)
```


For d = 99:

```{r}
set.seed(17)
d = 99
n = 100
betas99 <- matrix(nrow = 1000, ncol = d + 1)
betas99 <- as.data.frame(betas99)

# run the simulation with n and d 1000 times
for (i in 1:1000){
  betas99[i,] <- simulation(n, d)
}

# empty data frame
var99 <- matrix(nrow = 1, ncol = d + 1)
var99 <- as.data.frame(var99)

# calculate variance of each estimator
var99[,1] <- var(betas99[,1])
for (i in 1: d + 1){
  var99[,i] <- var(betas99[,i])
}

head(betas99)
head(var99)

```

We notice that the MSE of the Beta estimates (which is equal to the variance, since these estimates are unbiased by default (OLS estimators)) increases drastically when we set the number of parameters = n. This shows that when the number of parameters is very close to the sample size, we lose consistency of the Beta estimators. When we set the number of parameters = n+1, our code crashes. 

## Create some plots with the MSE

```{r}
library(ggplot2)
library(dplyr)
library(tidyverse)

MSE_beta1 <- c(var1[,2], var50[,2], var75[,2], var97[,2], var98[,2], var99[,2])
MSE_beta14 <- c(NA, var50[,15], var75[,15], var97[,15], var98[,15], var99[,15])
MSE_beta26 <- c(NA, var50[,27], var75[,27], var97[,27], var98[,27], var99[,27])
nb <- c(1,50,75, 97, 98,99)

temp <- data.frame(MSE1 = MSE_beta1, 
                   MSE26 = MSE_beta26,
                   nb = nb) %>%
  gather(key = "MSE", "value", 1:2)


temp %>% ggplot() +
  geom_point(aes(x = nb, y = value, color = MSE)) +
  geom_line(aes(x = nb, y = value, color = MSE)) +
  labs(x = "Number of Slope Parameters", y = "MSE (log scale)") + 
  ggtitle("MSE vs. Number of Slope Parameters (n = 100)")+
  scale_y_continuous(trans = "log10")+
  scale_color_discrete(name = expression(paste(beta)), 
                       labels = c(expression(paste(beta, 1)), expression(paste(beta, 26))))
```

```{r}
MSE_beta1 <- c(var1[,2], var50[,2], var75[,2], var97[,2], var98[,2])
#MSE_beta14 <- c(NA, var50[,15], var75[,15], var97[,15], var98[,15])
MSE_beta26 <- c(NA, var50[,27], var75[,27], var97[,27], var98[,27])
nb <- c(1,50,75, 97, 98)

temp <- data.frame(MSE1 = MSE_beta1, 
                   #MSE14 = MSE_beta14,
                   MSE26 = MSE_beta26,
                   nb = nb) %>%
  gather(key = "MSE", "value", 1:2)


temp %>% ggplot() +
  geom_point(aes(x = nb, y = value, color = MSE)) +
  geom_line(aes(x = nb, y = value, color = MSE)) +
  labs(x = "Number of Slope Parameters", y = "MSE (log scale)") + 
  ggtitle("MSE vs. Number of Slope Parameters (n = 100)")+
  scale_y_continuous(trans = "log10")+
  scale_color_discrete(name = expression(paste(beta)), 
                       labels = c(expression(paste(beta, 1)), expression(paste(beta, 26))))
```


## Put all variance/MSE results into a table:

```{r}
library(kableExtra)
library(pander)

mse_table <- rbind(var50[,1:8], var75[,1:8], var97[,1:8], var98[,1:8], var99[,1:8]) %>%
  round(2)

rownames(mse_table) <- c("d = 50   ", "d = 75   ", "d = 97   ", "d = 98   ", "d = 99   ")

knitr::kable(mse_table,
  col.names = c("$MSE(\\beta_0)$", "$MSE(\\beta_1)$", "$MSE(\\beta_2)$", "$MSE(\\beta_3)$",
                "$MSE(\\beta_4)$", "$MSE(\\beta_5)$", "$MSE(\\beta_6)$", "$MSE(\\beta_7)$"),
  row.names = TRUE,
  escape = FALSE) 
```

